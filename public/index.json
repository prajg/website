[{"authors":["admin"],"categories":null,"content":"I am currently a final year undergraduate student in Statistics, Mathematics and Actuarial Science at the University of Toronto. Within academia, I am a research assistant at RiskLab, and a teaching assistant in the Deparment of Statistical Sciences as well as Rotman School of Management.\nStarting Fall 2020, I will be joining Dynamic Optimization \u0026amp; Operations Management Lab in the MIE department to work on Reinforcement Learning and Quantitative Finance. In the lab, I will be working under the supervision of Professor Chi Guhn Lee.\nRecent coursework: Probabilistic Machine Learning, Stochastic Calculus for Asset Pricing, Statistical Methods for Machine Learning, Bayesian Statistics and Stochastic Processes\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/raj-patel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/raj-patel/","section":"authors","summary":"I am currently a final year undergraduate student in Statistics, Mathematics and Actuarial Science at the University of Toronto. Within academia, I am a research assistant at RiskLab, and a teaching assistant in the Deparment of Statistical Sciences as well as Rotman School of Management.","tags":null,"title":"Raj Patel","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1461110400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1555459200,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"faa3bb54beba6a92a2cbe2c6d48b6f13","permalink":"/courses/example2/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example2/","section":"courses","summary":"Statistical Methods for Machine Learning 2","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Assignment 1","type":"docs"},{"authors":null,"categories":null,"content":"Question 1 If we write down the model using the $\\hat{\\beta}$ coefficients we are provided, we have:\n$$ \\hat{y} = 50 + 20*GPA + 0.07*IQ + 35*Gender + 0.01*(GPA \\times IQ) - 10*(GPA \\times Gender) $$\nwhere Gender is 1 for Female and 0 for Male and $\\hat{y}$ represents salary in thousands\nSo, if we write separate models for male and female, we have:\nFor Male, the model is:\n$$ \\hat{y} = 50 + 20*GPA + 0.07*IQ + 0.01*(GPA \\times IQ) $$ For Female, the model is:\n$$ \\hat{y} = 85 + 10*GPA + 0.07*IQ + 0.01*(GPA \\times IQ) $$\nPart a: Here, all the options are comparing Male\u0026rsquo;s Salary with respect to Female\u0026rsquo;s.\nSo, firstly, let\u0026rsquo;s just equate both the models to see how does the inequality work out. If we want to see when, on average, do female earn more than male, the inequality we have is:\n$$ \\begin{aligned} \u0026amp; 85 + 10*GPA + 0.07*IQ + 0.01*(GPA \\times IQ) \u0026gt; 50 + 20*GPA + 0.07*IQ + 0.01*(GPA \\times IQ) \\\\\n\u0026amp; 85 + 10*GPA \u0026gt; 50 + 20*GPA \\\\\n\u0026amp; 3.5 \u0026gt; GPA \\end{aligned} $$\nTherefore, when the GPA is smaller than 3.5, females, on average, earn more than males.\n(i) is incorrect as males, on average, earn more than females only when GPA is greater than 3.5, and not in general for any constant value of GPA.\n(ii) is incorrect as females, on average, earn more than males only when GPA is less than 3.5, and not in general for any constant value of GPA.\n(iii) is correct as males, on average, earn more than females when GPA is greater than 3.5. So, if we quantify high GPA as GPA \u0026gt;3.5, this statement is correct.\n(iv) is incorrect as for high GPA, females earn less than males, on average.\nPart b: Here, we want to predict the salary of a female with IQ 110 and GPA 4.0. So, here, if we plug in the values in our base model, we have:\n$$ \\begin{aligned} \\hat{Y} \u0026amp;= 85 + 10*GPA + 0.07*IQ + 0.01*(GPA \\times IQ) \\\\\n\u0026amp;= 85 + 10*4 + 0.07*110 + 0.01*(4*110) \\\\ \u0026amp;= 137.1 \\end{aligned} $$\nPart c: False. To verify if (GPA \u0026amp; IQ) together have an impact, we need to test $H_0: \\hat{\\beta_4} = 0$ and look at the p-value associated with a t-statistic or an F-statistic to come to a conclusion.\nIf we notice partial effect of IQ and its interactive term, we see that IQ\u0026rsquo;s main effect is (0.07 * I**Q) and the effect of interactive term is (0.01 * GPA * I**Q) . So, IQ hierarchical effect is (GPA/7) , which is around 57% the above case for GPA = 4.0. So, we can not ignore it before having a closer look at the respective p-value.\nQuestion 2 Part a: By installing the library ISLR, we already have the dataset Carseats. So, we will directly jump to fitting the model.\nmod1 \u0026lt;- lm(Sales ~ Price + Urban + US, data = Carseats) summary(mod1) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 \u0026lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 \u0026lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: \u0026lt; 2.2e-16  Part b: To interpret the coefficients, we first look at the categorical variables using contrast function\nknitr::kable(contrasts(Carseats$Urban), caption = \u0026quot;Coding R uses in case of Urban vs Rural\u0026quot;)   Coding R uses in case of Urban vs Rural   Yes    No 0  Yes 1    knitr::kable(contrasts(Carseats$US), caption = \u0026quot;Coding R uses in case of US vs Non US\u0026quot;)   Coding R uses in case of US vs Non US   Yes    No 0  Yes 1    Based on the result of contrast function, we observe that person living in urban setting has value 1 for Urban variable and person living in rural setting has value 0 for the same. For US variable, R assigns value 1 if the store is in US and 0 if it is not.\nNow that we know the qualitative predictors, we can start interpreting the coefficients in the model.\n Intercept represents sales(in thousands) for a store where it does not charge any price for car seats, which is in a rural setting and the store is not in US. For the Price coefficient, we can say that for every unit increase in the price charged for car seats, the sales decrease by approx 54.45 units(0.05445*1000) keeping all other predictors fixed. Coefficient for UrbanYes indicates the average difference in the sales of a store located in Urban area as compared to rural area. So, if the store is in Urban area, then keeping everything else constant, the sales go down by 21.9 units as compared to stores in Rural area. However, since the p-value is not significant, we can not be certain about this relationship. Lastly, the USYes coefficient can be interpreted as the average increase in Sales provided that the store is located in the United States. Thus, on average, the sales in a US store are 1200.57 units more than in a non US store keeping all other predictors remaining fixed.  Part c: Sales = 13.043469 − 0.054459 * Price − 0.021916 * UrbanYe**s + 1.200573 * USYes + ϵ where, UrbanYes is 1 if the store is in Urban setting and 0 if not while USYes is 1 if the store is in US and 0 if not\nPart d: For rejecting the null hypothesis H0 : βj = 0, we would want a small p-value (less than 0.05). So, here we can reject null hypothesis for Price and USYes variable as their p-value is less than 0.05. A small p-value indicates that we observe an association between the predictor and the response. So, we reject the null hypothesis and declare a relationship to exist between predictor and response.\nPart e: On the basis of previous question, we now want to fit a new smaller model that only uses the predictors for which there is evidence of association with the response.\nmod2 \u0026lt;- lm(Sales ~ Price + US, data = Carseats) summary(mod2) ## ## Call: ## lm(formula = Sales ~ Price + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9269 -1.6286 -0.0574 1.5766 7.0515 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 13.03079 0.63098 20.652 \u0026lt; 2e-16 *** ## Price -0.05448 0.00523 -10.416 \u0026lt; 2e-16 *** ## USYes 1.19964 0.25846 4.641 4.71e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.469 on 397 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 ## F-statistic: 62.43 on 2 and 397 DF, p-value: \u0026lt; 2.2e-16  Part f: On comparing the output of summaries, we see that R2 is the same for both the models. So, while going from smaller to bigger model (i.e., model in part (e) to model in part (a)), because the R2 does not increase, we can continue with the smaller model and drop the Price$Urban variable as it is not helping improving the fit.\nMoreover, while going from smaller model to base model, RSE increases, F-statistic decreases and the Adjusted R2 decreases a bit as well. So, we can say that smaller model would be marginally better for all the above mentioned reasons and it is also easier to interpret.\nBut on average, if we just look at the summary statistics for model comparison, the models are quite similar.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"14f6b6daaa63f0f80f6c55a2500c2f24","permalink":"/courses/example/assignment_2_updated/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/assignment_2_updated/","section":"courses","summary":"Question 1 If we write down the model using the $\\hat{\\beta}$ coefficients we are provided, we have:\n$$ \\hat{y} = 50 + 20*GPA + 0.07*IQ + 35*Gender + 0.01*(GPA \\times IQ) - 10*(GPA \\times Gender) $$","tags":null,"title":"Assignment 2","type":"docs"},{"authors":null,"categories":null,"content":"Question 1: Part a: Write down what is P(X|Dividend = Yes). P(X|Dividend = Yes), based on the question, follows a normal distribution with mean = 10 and variance = 36.\nSo, it\u0026rsquo;s density would look like: $$ \\begin{aligned} f(x) \u0026amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x - \\mu)^2}{{2\\sigma^2}}) \\\\\n\u0026amp;= \\frac{1}{\\sqrt{2\\pi\\times 36}}exp(-\\frac{1}{{2\\sigma^2}}(x - 10)^2) \\end{aligned} $$\nPart b: Write down what is P(X|Dividend = No). P(X|Dividend = No), based on the question, also follows a normal distribution with mean = 0 and variance = 36.\nSo, it\u0026rsquo;s density would look like: $$ \\begin{aligned} f(x) \u0026amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x - \\mu)^2}{{2\\sigma^2}}) \\\\\n\u0026amp;= \\frac{1}{\\sqrt{2\\pi\\times 36}}exp(-\\frac{x^2}{{2\\sigma^2}}) \\end{aligned} $$\nPart c: Use dnorm() to calculate conditional probabilities in a) and b) when X=4. Here, we will firstly calculate conditional probability for part(a) when X=4.\ndnorm(4, mean = 10, sd = 6, log = FALSE) ## [1] 0.04032845  Based on this, we see that the conditional probability in (a) when X=4 is 0.04032845. Now, we will be calculating conditional probability for part(b) when X=4.\ndnorm(4, mean = 0, sd = 6, log = FALSE) ## [1] 0.05324133  Based on this, we see that the conditional probability in (b) when X=4 is 0.05324133.\nPart d: What is the value of P(Dividend = Yes) Based on the question, we are given that 80% of the companies issue dividends. So, P(Dividend = Yes) = 0.8\nPart e: Write down what is P(X|Dividend = No). Based on the question, we know that 20% of the companies do not issue dividends. So, P(Dividend = No) = 0.2\nPart f: Predict the probability that a company will issue a dividend this year given its percentage profit was X = 4. Based on part (c), (d) and (e), we will predict the probability that a company will issue a dividend this year given its percentage profit was X = 4. To do this, we will be using Bayes\u0026rsquo; rule. Based on that, we get probability = 0.752.\n$$ \\begin{aligned} p_{yes}(x) \u0026amp;= \\frac{\\pi_{yes}exp(-\\frac{(x - \\mu_{yes})^2}{{2\\sigma^2}})}{\\pi_{yes}exp(-\\frac{(x - \\mu_{yes})^2}{{2\\sigma^2}}) + \\pi_{no}exp(-\\frac{(x - \\mu_{no})^2}{{2\\sigma^2}})} \\\\\n\u0026amp;= \\frac{0.8 \\times 0.04032845}{0.8 \\times 0.04032845 + 0.2 \\times 0.05324133}\\\\\n\u0026amp;= 0.752 \\end{aligned} $$\nQuestion 2: Part a: Here, we want to create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median.\n# Check the first five entries of the dataset to see if the dataset is loaded properly knitr::kable(head(Auto))    mpg cylinders displacement horsepower weight acceleration year origin name    18 8 307 130 3504 12.0 70 1 chevrolet chevelle malibu  15 8 350 165 3693 11.5 70 1 buick skylark 320  18 8 318 150 3436 11.0 70 1 plymouth satellite  16 8 304 150 3433 12.0 70 1 amc rebel sst  17 8 302 140 3449 10.5 70 1 ford torino  15 8 429 198 4341 10.0 70 1 ford galaxie 500    # Creating a variable \u0026quot;mpg01\u0026quot; whose value is 1 if the corresponding mpg value is greater # than median mpg value and 0 if less than the median mpg01 \u0026lt;- rep(0, length(Auto$mpg)) mpg01[Auto$mpg \u0026gt; median(Auto$mpg)] \u0026lt;- 1 # Merging the \u0026quot;mpg01\u0026quot; variable with other variables in the Auto dataset Auto \u0026lt;- data.frame(Auto, mpg01)  Part b: Here, we want to see which continuous features seem most likely to be useful in predicting the mpg. We are provided in the question that we should use cor() function here and consider features with correlation coefficient greater than 0.6 in predicting.\n# Because we can use cor() function only on numeric values, we remove the 9th column, # which is the \u0026quot;Names\u0026quot; of the vehicles/cars A \u0026lt;- cor(Auto[,-9]) A ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 ## mpg01 0.8369392 -0.7591939 -0.7534766 -0.6670526 -0.7577566 ## acceleration year origin mpg01 ## mpg 0.4233285 0.5805410 0.5652088 0.8369392 ## cylinders -0.5046834 -0.3456474 -0.5689316 -0.7591939 ## displacement -0.5438005 -0.3698552 -0.6145351 -0.7534766 ## horsepower -0.6891955 -0.4163615 -0.4551715 -0.6670526 ## weight -0.4168392 -0.3091199 -0.5850054 -0.7577566 ## acceleration 1.0000000 0.2903161 0.2127458 0.3468215 ## year 0.2903161 1.0000000 0.1815277 0.4299042 ## origin 0.2127458 0.1815277 1.0000000 0.5136984 ## mpg01 0.3468215 0.4299042 0.5136984 1.0000000 library(corrplot) corrplot(A, type=\u0026quot;full\u0026quot;, method = \u0026quot;color\u0026quot;)  Correlation plot Based on the correlation table and the figure, we can say that \u0026ldquo;cylinders\u0026rdquo;, \u0026ldquo;displacement\u0026rdquo;, \u0026ldquo;horsepower\u0026rdquo; and \u0026ldquo;weight\u0026rdquo; are four features that we will be using for prediction as their absolute correlation coefficient is greater than 0.6.\nJust to confirm the impact of the features above, we plot individual box plots which confirm the same.\npar(mfrow=c(2,2)) boxplot(Auto$cylinders ~ Auto$mpg01, main = \u0026quot;Cylinders vs mpg01\u0026quot;) boxplot(Auto$displacement ~ Auto$mpg01, main = \u0026quot;Displacement vs mpg01\u0026quot;) boxplot(Auto$horsepower ~ Auto$mpg01, main = \u0026quot;Horsepower vs mpg01\u0026quot;) boxplot(Auto$weight ~ Auto$mpg01, main = \u0026quot;Weight vs mpg01\u0026quot;)  Box-plots of features to be used in prediction Part c: Here, we want to split the data into training and test set, holding 30% for the test set. So, for that, we use the sample.split function and then store them in Auto.train and Auto.test respectively.\nset.seed(101) temp \u0026lt;- sample.split(Auto, SplitRatio = 0.7) Auto.train \u0026lt;- Auto[temp,] Auto.test \u0026lt;- Auto[!temp,]  Part d: Here, we want to perform LDA using the variables that seemed most associated in part b and then want to find the test error.\nset.seed(101) # Fitting the LDA model mod_lda \u0026lt;- lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train) mod_lda ## Call: ## lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train) ## ## Prior probabilities of groups: ## 0 1 ## 0.4981818 0.5018182 ## ## Group means: ## cylinders weight displacement horsepower ## 0 6.678832 3605.263 269.2482 129.30657 ## 1 4.224638 2357.543 118.4312 79.65217 ## ## Coefficients of linear discriminants: ## LD1 ## cylinders -0.289911365 ## weight -0.001216868 ## displacement -0.002048940 ## horsepower 0.004664771 # Predicting based on the fitted LDA model pred.lda \u0026lt;- predict(mod_lda, Auto.test) # Generating the confusion matrix to check the error rate. table(pred.lda$class, Auto.test[,\u0026quot;mpg01\u0026quot;]) ## ## 0 1 ## 0 51 1 ## 1 8 57 mean(pred.lda$class != Auto.test[,\u0026quot;mpg01\u0026quot;]) ## [1] 0.07692308  Based on this, we can say that the test error is 7.69% in the case of using LDA on features selected in part b.\nPart e: Here, we want to perform QDA using the variables that seemed most associated in part b and then want to find the test error.\nset.seed(101) # Fitting the LDA model mod_qda \u0026lt;- qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train) mod_qda ## Call: ## qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train) ## ## Prior probabilities of groups: ## 0 1 ## 0.4981818 0.5018182 ## ## Group means: ## cylinders weight displacement horsepower ## 0 6.678832 3605.263 269.2482 129.30657 ## 1 4.224638 2357.543 118.4312 79.65217 # Predicting based on the fitted LDA model pred.qda \u0026lt;- predict(mod_qda, Auto.test) # Generating the confusion matrix to check the error rate. table(pred.qda$class, Auto.test[,\u0026quot;mpg01\u0026quot;]) ## ## 0 1 ## 0 54 1 ## 1 5 57 mean(pred.qda$class != Auto.test[,\u0026quot;mpg01\u0026quot;]) ## [1] 0.05128205  Based on this, we can say that the test error is 5.12% in the case of using QDA on features selected in part b.\nPart f: set.seed(101) glm_mod \u0026lt;- glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train, family = binomial) summary(glm_mod) ## ## Call: ## glm(formula = mpg01 ~ cylinders + weight + displacement + horsepower, ## family = binomial, data = Auto.train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3047 -0.2763 0.1173 0.3913 3.1478 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 10.6027057 1.8659851 5.682 1.33e-08 *** ## cylinders 0.2606786 0.3991895 0.653 0.51374 ## weight -0.0020693 0.0007444 -2.780 0.00544 ** ## displacement -0.0136283 0.0092348 -1.476 0.14001 ## horsepower -0.0383627 0.0156556 -2.450 0.01427 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 381.23 on 274 degrees of freedom ## Residual deviance: 162.40 on 270 degrees of freedom ## AIC: 172.4 ## ## Number of Fisher Scoring iterations: 7 set.seed(101) # getting the predicted probabilities probs \u0026lt;- predict(glm_mod, Auto.test, type = \u0026quot;response\u0026quot;) # creating the confusion matrix pred.glm \u0026lt;- rep(0, length(probs)) pred.glm[probs \u0026gt; 0.5] \u0026lt;- 1 table(pred.glm, Auto.test$mpg01) ## ## pred.glm 0 1 ## 0 53 2 ## 1 6 56 mean(pred.glm != Auto.test$mpg01) ## [1] 0.06837607  Based on this, we can say that the test error is 6.84% in the case of using Logistic Regression on features selected in part b.\nset.seed(101) library(class) train.X = cbind(Auto.train$cylinders, Auto.train$weight, Auto.train$displacement, Auto.train$horsepower) test.X = cbind(Auto.test$cylinders, Auto.test$weight, Auto.test$displacement, Auto.test$horsepower) train.Y = Auto.train$mpg01 test.Y = Auto.test$mpg01 knn_test_error = c(); for ( i in 1:200 ) { knn.pred = knn(train.X, test.X, train.Y, k = i) knn_test_error[i] = mean(knn.pred != test.Y)*100 } plot(1:200, knn_test_error, xlab = \u0026quot;K\u0026quot;, ylab = \u0026quot;Test Error Rate (%)\u0026quot;)  df \u0026lt;- data.frame(matrix(ncol = 2, nrow = 10)) x \u0026lt;- c(\u0026quot;K\u0026quot;, \u0026quot;Error\u0026quot;) colnames(df) \u0026lt;- x temp_vec \u0026lt;- c(1,5,10,15,20,30,50,100,150,200) for(i in 1:10){ df[i,1] \u0026lt;- temp_vec[i] } for(j in 1:10){ df[j,2] \u0026lt;- knn_test_error[temp_vec[j]] } knitr::kable(df, digits = 3)    K Error    1 11.111  5 9.402  10 10.256  15 10.256  20 10.256  30 8.547  50 9.402  100 9.402  150 9.402  200 11.966    which.min(knn_test_error) ## [1] 7 min(knn_test_error) ## [1] 7.692308  So, from the values provided in question, based on the above table, we see that KNN performs best for K=30 with test error rate of 8.547%.\nHowever, if we take all values from 1 to 200, when K = 7, KNN classifier performs the best with the test error rate = 7.69%.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6c0b2336972edcd382593bbda5a75122","permalink":"/courses/example/assignment_3_updated/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/assignment_3_updated/","section":"courses","summary":"Question 1: Part a: Write down what is P(X|Dividend = Yes). P(X|Dividend = Yes), based on the question, follows a normal distribution with mean = 10 and variance = 36.","tags":null,"title":"Assignment 3","type":"docs"},{"authors":null,"categories":null,"content":"Question 1 Here, y1, y2, …, yn is a random sample from a Bernoulli distribution where Y ∼ Ber**n(p) with pmf of Y: f(yi|p)=pyi(1 − p)1 − yi; yi ∈ {0, 1; 1 = success}\nPart a: Here, we want to find the MLE of p. So, here, the likelihood function is: $$ L(p) = \\prod_{i = 1}^{n} p^{y_i} (1-p)^{1-y_i} $$ So, from this, log-likelihood is: $$ l(p) = \\log{p} \\sum_{i = 1}^{n} y_i + \\log{(1-p)} \\sum_{i = 1}^{n} (1 - y_i) $$ Now, to find the MLE of p, we set the first derivative of l(p) to be zero and double check to make sure that the second derivative is negative.\nIf we set the first derivative to be zero, we have: $$ \\frac{dl(p)}{dp} = \\frac{\\sum_{i = 1}^{n} y_i}{p} - \\frac{\\sum_{i = 1}^{n} (1 - y_i)}{(1-p)} = 0$$ $$ \\therefore \\sum_{i = 1}^{n} y_i - p \\sum_{i = 1}^{n} y_i = p \\sum_{i = 1}^{n} (1-y_i)$$ $$ \\therefore \\hat{p} = \\frac{\\sum_{i = 1}^{n} y_i}{n}$$ Now, to make sure that this is the maximum likelihood estimator, we double check that the second derivative is negative. $$ \\displaystyle \\frac{d^2l(p)}{dp^2} = - \\frac {\\sum_{i = 1}^{n} y_i}{p^2} - \\frac{\\sum_{i = 1}^{n} (1-y_i)}{(1-p)^2}$$ Now, here, p ∈ [0, 1] and y ∈ {0, 1}. Therefore, the second derivative is negative as both the terms will be negative.\nPart b: Here, we are given that in five independent Bernoulli trials from above Bernoulli process, three successes and two failures were observed. Calculate the maximum likelihood estimates of p in this situation.\nFrom part a, we know that $\\hat{p} = \\frac{\\sum_{i = 1}^{n} y_i}{n}$.\nHere, because we had 3 successes and 2 failures, $\\sum_{i = 1}^{n} y_i = 3$ as in the question, we are given that 1 = success and 0 = failure. Also, because there are 5 trials, we have n = 5.\n$$ \\therefore \\hat{p} = \\frac{\\sum_{i = 1}^{n} y_i}{n} = \\frac{3}{5}$$ .\nPart c: Here, we want to plot the log-likelihood for the data in part(b) by performing a grid search over a set of possible values of p parameter. Then, we want to add a vertical line to the plot at the value of p that maximizes the log-likelihood.\nFirstly, we will be creating a log-likelihood function. Then, we define the possible set of values our parameter p can take. Once we have that, we will find the loglikelihood for all possible parameter values and then find p for which it is the largest.\nWhile doing all of this, we will plot the log-likelihood function and then add a vertical line to the plot at the value of p that maximizes the log-likelihood.\n# Defining the log-likelihood function log_lik_func \u0026lt;- function(x, p = 3, n = 5) { p*log(x) + (n-p)*log(1-x)} # Values our parameter can take param_values \u0026lt;- seq(0,1, by = 0.2) # Finding log-likelihood for all possible values of parameter val \u0026lt;- sapply(param_values , log_lik_func) val ## [1] -Inf -5.274601 -3.770523 -3.365058 -3.888306 -Inf # Parameter value for which we have maximum log-likelihood param_values[which.max(val)] ## [1] 0.6 # plot of log-likelihood curve(log_lik_func, from = 0, to = 1) points(param_values, val, col = 'blue', pch = 20) points(param_values[which.max(val)], val[which.max(val)], col = 'red', pch = 19) abline(v = param_values[which.max(val)], col = 'red')  Question 2: Here, we have a case of simple logistic regression where Y is a binary dependent variable and X is the predictor variable with sample data (x1, y1),(x2, y2),…,(xn, yn). Binary outcomes are modelled as Bernoulli trials as in Question 1 above. Moreover, f(yi|pi)=piyi(1 − pi)1 − yi; yi ∈ {0, 1; 1 = Yes} $$ p_i = \\frac{e^{{\\beta_0}+{\\beta_1x_i}}}{1+e^{{\\beta_0}+{\\beta_1x_i}}} $$\nPart a: Here, we want to derive the log-likelihood function l(β), where β = (β0, β1).\nSo, here, firstly our likelihood function is as follows: $$ L(\\beta) = \\prod_{i=1}^{n}p_i^{y_i} (1-p_i)^{1-y_i} $$ So, from this, our log-likelihood function is as follows: $$ \\begin{aligned} l(\\beta_0,\\beta_1) \u0026amp;= \\sum_{i=1}^{n} y_i \\log(p_i) + \\sum_{i=1}^{n} (1-y_i) \\log(1 - p_i) \\\\\n\u0026amp;= \\sum_{i=1}^{n} \\log(1-p_i) + \\sum_{i=1}^{n} y_i \\log\\frac{p_i}{(1 - p_i)} \\\\\n\u0026amp;= \\sum_{i=1}^{n} \\log(1-p_i) + \\sum_{i=1}^{n} y_i (\\beta_0 + \\beta_1 x_i) \\\\\n\u0026amp;= \\sum_{i=1}^{n} \\log(1-(\\frac{e^{{\\beta_0}+{\\beta_1x_i}}}{1+e^{{\\beta_0}+{\\beta_1x_i}}})) + \\sum_{i=1}^{n} y_i (\\beta_0 + \\beta_1 x_i) \\\\\n\u0026amp;= \\sum_{i=1}^{n} \\log(\\frac{1}{1+e^{{\\beta_0}+{\\beta_1x_i}}}) + \\sum_{i=1}^{n} y_i (\\beta_0 + \\beta_1 x_i) \\\\\n\u0026amp;= \\sum_{i=1}^{n} \\log{1} - \\log{(1+e^{{\\beta_0}+{\\beta_1x_i}}}) + \\sum_{i=1}^{n} y_i (\\beta_0 + \\beta_1 x_i) \\\\\n\\end{aligned} $$\nNow, log1 = 0. So, our log-likelihood function is: $$ l(\\beta_0,\\beta_1) = -\\sum_{i=1}^{n}\\log{(1+e^{{\\beta_0}+{\\beta_1x_i}}}) + \\sum_{i=1}^{n} y_i (\\beta_0 + \\beta_1 x_i) $$\nPart b: Here, we want to write function ll() which calculates the log-likelihood. For this, we will be using our result from part a.\nHere, in the output, we add a negative sign to the function we get from part a. This is because we will be using this function in optim() function. Now, optim() minimizes the function by default. But we want parameters that maximize it.\nSo, we will be returning \u0026ldquo;-log-likelihood\u0026rdquo; which on minimizing will give the values of parameter which maximize log-likelihood.\nll \u0026lt;- function(x, y, beta) { # parameter 1 b_0 = beta[1] # parameter 2 b_1 = beta[2] n = length(x) t \u0026lt;- replicate(length(x), 0) t2 \u0026lt;- replicate(length(x), 0) # term 1 from the log-likelihood function from part a: term_1 = 0 # term 2 from the log-likelihood function from part a: term_2 = 0 for(i in 1:n){ t[i] = -log(1 + exp(b_0 + ((b_1)*x[i]))) term_1 = term_1 + t[i] t2[i] = y[i]*(b_0 + (b_1*x[i])) term_2 = term_2 + t2[i] } # This is our log-likelihood function log_likelihood \u0026lt;- (term_1 + term_2) # Later, we will be using this function in optim() function. Now, optim() minimizes the # function by default. So, we will be returning \u0026quot;-log-likelihood\u0026quot; which on minimizing # will give the values of parameter which maximize log-likelihood -(term_1 + term_2) }  Part c: Here, using the default dataset in the book, we want to fit a logistic regression model to predict default given balance as a predictor using glm().\n# model generation glm_mod \u0026lt;- glm(default ~ balance, data = Default, family = binomial) summary(glm_mod) ## ## Call: ## glm(formula = default ~ balance, family = binomial, data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2697 -0.1465 -0.0589 -0.0221 3.7589 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.065e+01 3.612e-01 -29.49 \u0026lt;2e-16 *** ## balance 5.499e-03 2.204e-04 24.95 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1596.5 on 9998 degrees of freedom ## AIC: 1600.5 ## ## Number of Fisher Scoring iterations: 8  Part d: Here, we want to use optim() function together with ll() and initial parameter estimates as zero to calculate maximum likelihood estimates of regression coefficients in part c.\nIn our data, the response variable \u0026ldquo;default\u0026rdquo; is of datatype factor. As a result, we will first convert that to 0\u0026rsquo;s and 1\u0026rsquo;s with 1 being default and 0 being non-default.\nFollowing that, we will convert this to a vector as manipulating factors is a bit tricky.\nFinally, we use optim function to find the parameter values which maximize log-likelihood function starting from (0,0) as the parameter values. By default, optim function minimizes the function but we wanted to find parameter values which maximize the log-likelihood. As a result, we earlier added a negative sign in the output of log-likelihood so that the result given by optim function fits our requirement.\n# Convert factor levels to 0 and 1 output_default \u0026lt;- Default$default levels(output_default)[1]\u0026lt;-\u0026quot;0\u0026quot; levels(output_default)[2]\u0026lt;-\u0026quot;1\u0026quot; # Change to a vector output_default \u0026lt;- as.numeric(as.character(output_default)) # Using optim function on log-likelihood to find parameters, starting from (0,0) optim(c(0,0), ll,x = Default$balance, y = output_default) ## $par ## [1] -10.652058220 0.005499188 ## ## $value ## [1] 798.2259 ## ## $counts ## function gradient ## 107 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL  Part e: Based on our outputs from part (c) and part (d), we can say that maximum likelihood estimates obtained using optim() function are very similar to the ones obtained using glm() function.\nPart f: Here, we want to calculate the standard errors of our estimates. As provided in the question, we can include parameter option \u0026lsquo;hessian = TRUE\u0026rsquo; in the function optim() when we call the function optim() in part d.\nBut here, , we see that if we invert the Hessian matrix using the default method in optim, we get negative numbers on the diagonal. So, taking their square roots will not be possible.\nAs a result, we will use Quasi Newton method, specifically the \u0026lsquo;BFGS\u0026rsquo; method as those methods are used when Hessian is unavailable.\nHessian_mat \u0026lt;- optim(c(0,0), ll,x = Default$balance, y = output_default, hessian = TRUE, method = 'BFGS')$hessian # Now, we find the inverse of this and then we take the square root of the diagonal. std_err_est \u0026lt;- sqrt(diag(solve(Hessian_mat))) std_err_est ## [1] 0.3378529809 0.0002296589  Part g: Bases on the outputs we have from part (c) and (f), we can say that standard errors of the MLE obtained using optim() and glm() are very similar.\nQuestion 3 Part a: Here, using the summary and glm functions, we want to determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model.\nset.seed(100) fit1 \u0026lt;- glm(default ~ income + balance, data=Default, family=binomial) summary(fit1) ## ## Call: ## glm(formula = default ~ income + balance, family = binomial, ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4725 -0.1444 -0.0574 -0.0211 3.7245 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.154e+01 4.348e-01 -26.545 \u0026lt; 2e-16 *** ## income 2.081e-05 4.985e-06 4.174 2.99e-05 *** ## balance 5.647e-03 2.274e-04 24.836 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1579.0 on 9997 degrees of freedom ## AIC: 1585 ## ## Number of Fisher Scoring iterations: 8 sum_coef \u0026lt;- rbind(summary(fit1)$coef[1:3,1:2]) knitr::kable(sum_coef, caption = \u0026quot;Estimate and Standard Error of the model\u0026quot;)   Estimate and Standard Error of the model   Estimate Std. Error    (Intercept) -11.5404684 0.4347564  income 0.0000208 0.0000050  balance 0.0056471 0.0002274    Therefore, the estimated standard errors for β0 is 4.347564e-01, for β1, which is the coefficient associated with income, is 4.985167e-06 and that for β2, which is the coefficient associated with balance, is 2.273731e-04\nPart b: Here, we want to write a function that takes as input the Default data set as well as index of observations and then outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nset.seed(100) boot.fn \u0026lt;- function(data_set, index){ fit2 \u0026lt;- glm(default ~ income + balance, data=data_set, subset = index, family=binomial) return(coef(fit2)) }  Part c: Here, we want to use the boot() function together with our boot.fn() to estimate the standard errors of the logistic regression coefficients for income and balance.\nFor R = 1000 replications\npar(mfrow = c(3,2)) set.seed(100) results \u0026lt;- boot(Default, boot.fn, 1000) results ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Default, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -1.154047e+01 -2.927307e-02 4.446538e-01 ## t2* 2.080898e-05 -3.481513e-08 4.916633e-06 ## t3* 5.647103e-03 1.665079e-05 2.318937e-04  So, here, standard error for β0 is 4.276469e-01, for β1, which is the coefficient associated with income, is 4.890019e-06 and that for β2, which is the coefficient associated with balance, is 2.250900e-04\nPart d: Here, we want to comment on the estimated standard errors obtained using the glm() function and using bootstrap function.\nSo, based on the results we have, the estimated standard errors obtained by the two methods are pretty close.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"a2b8fdf45c48fff23825fd25554cd880","permalink":"/courses/example/assignment_4_updated/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/assignment_4_updated/","section":"courses","summary":"Question 1 Here, y1, y2, …, yn is a random sample from a Bernoulli distribution where Y ∼ Ber**n(p) with pmf of Y: f(yi|p)=pyi(1 − p)1 − yi; yi ∈ {0, 1; 1 = success}","tags":null,"title":"Assignment 4","type":"docs"},{"authors":null,"categories":null,"content":"Question 1 Here, we want to explore the fact that ridge regression tends to give similar coefficient values to correlated variables, whereas lasso may give quite different coefficient values to correlated variables.\nIn this question, we are provided that n = 2, p = 2, x11 = x12 and x21 = x22. Furthermore, we are also given that y1 + y2 = 0, x11 + x21 = 0 and x12 + x22 = 0. As a result, the estimate for the intercept in a least squares, ridge regression or lasso model is zero: $\\hat{\\beta_0} = 0$\nPart a: Here, we want to write out the ridge regression optimization problem in this setting.\nSo, we want to minimize:\n(y1 − β1x11 − β2x12)2 + (y2 − β1x21 − β2x22)2 + λ(β12 + β22)\nNow, we know that x11 = x12 and x21 = x22. So, substituting x12 as x11 and x22 as x21, our function to be minimized becomes,\nNow, we know that y1 + y2 = 0. Therefore, y2 = −y1. Moreover, we also know that x11 + x21 = 0. So, x21 = −x11. So, substituting this, our problem becomes,\n(y1 − β1x11 − β2x11)2 + ( − (y1 − β1x11 − β2x11))2 + λ(β12 + β22) which is,\n$$\\boxed{2(y_1 - ({\\beta_1} + {\\beta_2})x_{11})^2 + \\lambda ({\\beta_1}^2 + {\\beta_2}^2)}$$\nPart b: Here, to find the coefficient estimates for ridge regression, we first differentiate result (A) from part (a) with respect to $\\hat{\\beta_1}$ and $\\hat{\\beta_2}$, set both of them to zero and then solve for $\\hat{\\beta_1}$ and $\\hat{\\beta_2}$\nDifferentiating with respect to $\\hat{\\beta_1}$ and setting it to 0, we have: $$2(y_1-\\hat{\\beta_1}x_{11} - \\hat{\\beta_2}x_{11}) (-x_{11}) + 2(y_2-\\hat{\\beta_1}x_{21} - \\hat{\\beta_2}x_{21}) (-x_{21}) + 2\\lambda \\hat{\\beta_1} = 0$$\nTherefore,\nSimilarly, differentiating with respect to $\\hat{\\beta_2}$ and setting it to 0, we have: $$2(y_1-\\hat{\\beta_1}x_{11} - \\hat{\\beta_2}x_{11}) (-x_{11}) + 2(y_2-\\hat{\\beta_1}x_{21} - \\hat{\\beta_2}x_{21}) (-x_{21}) + 2\\lambda \\hat{\\beta_2} = 0$$\nTherefore,\nNow, to solve for $\\hat{\\beta_1}$ and $\\hat{\\beta_2}$, we compare equation (B) and equation (C). Comparing that, we get: $$\\hat{\\beta_1} = \\hat{\\beta_2}$$\nPart c: Here, we want to write out the lasso optimization problem in this setting.\nSo, we want to minimize:\n(y1 − β1x11 − β2x12)2 + (y2 − β1x21 − β2x22)2 + λ(|β1|+|β2|)\nNow, we know that x11 = x12 and x21 = x22. So, substituting x12 as x11 and x22 as x21, our function to be minimized becomes,\nNow, we know that y1 + y2 = 0. Therefore, y2 = −y1. Moreover, we also know that x11 + x21 = 0. So, x21 = −x11. So, substituting this, our problem becomes,\n(y1 − β1x11 − β2x11)2 + ( − (y1 − β1x11 − β2x11))2 + λ(|β1|+|β2|) which is,\n$$\\boxed{2(y_1 - ({\\beta_1} + {\\beta_2})x_{11})^2 + \\lambda (|{\\beta_1}| + |{\\beta_2}|)}$$\nPart d: Here, we want to show that for the optimization problem in part (c), the Lasso coefficients are not unique and then show the solutions. Now, in part (c), we see that the optimization problem has absolute values. So, understanding how to differentiate them is the first important step.\nFrom basic calculus, we know that:\n$$ \\frac{d}{{dx}}|u| = \\frac{d}{{dx}}\\sqrt {{{{u}}^2}} = \\frac {u \\times u\u0026rsquo;}{|u|}, u \\neq 0 $$\nSo, from part (c), we differentiate equation D with respect to $\\hat{\\beta_1}$ and $\\hat{\\beta_2}$ and set them to 0. Differentiating with respect to $\\hat{\\beta_1}$ and setting it to 0, we have: $$2(y_1-\\hat{\\beta_1}x_{11} - \\hat{\\beta_2}x_{11}) (-x_{11}) + 2(y_2-\\hat{\\beta_1}x_{21} - \\hat{\\beta_2}x_{21}) (-x_{21}) + \\frac{\\lambda \\hat{\\beta_1}}{|\\hat{\\beta_1|}} = 0$$\nSimilarly, differentiating with respect to $\\hat{\\beta_2}$ and setting it to 0, we have: $$2(y_1-\\hat{\\beta_1}x_{11} - \\hat{\\beta_2}x_{11}) (-x_{11}) + 2(y_2-\\hat{\\beta_1}x_{21} - \\hat{\\beta_2}x_{21}) (-x_{21}) + \\frac{\\lambda \\hat{\\beta_2}}{|\\hat{\\beta_2}|} = 0$$\nNow, equating the above two equations, we get: $$ \\frac{\\lambda \\hat{\\beta_1}}{|\\hat{\\beta_1}|} = \\frac{\\lambda \\hat{\\beta_2}}{|\\hat{\\beta_2}|} $$ which implies, $$ \\frac{\\hat{\\beta_1}}{|\\hat{\\beta_1}|} = \\frac{\\hat{\\beta_2}}{|\\hat{\\beta_2}|} $$\nfrom which we can see that both $\\hat{\\beta}_1$, $\\hat{\\beta}_2$ take the same sign and that there are many possible solutions to this optimization problem.\nNow, we know an alternative form of Lasso regression which is: $$ \\underset{\\boldsymbol{\\beta}}{\\mathrm{argmin}} ((y_1 - \\hat{\\beta_1}x_{11} - \\hat{\\beta_2}x_{12})^2 + (y_2 - \\hat{\\beta_1}x_{21} - \\hat{\\beta_2}x_{22})^2) $$ subject to $$| \\hat{\\beta}_1 | + | \\hat{\\beta}_2 | \\le s $$\nThe above form says that the lasso coefficients are the ones that have the smallest RSS out of all points that lie within the diamond defined by $| \\hat{\\beta}_1 | + | \\hat{\\beta}_2 | \\le s$.\nAnd the point with smallest RSS will be the first point at which the contours of RSS intersects the diamond defined by $| \\hat{\\beta}_1 | + | \\hat{\\beta}_2 | \\le s$.\nAs it is first point of intersection, it will be a point on diamond and so, our solution is: $| \\hat{\\beta}_1 | + | \\hat{\\beta}_2 | = s$, which can be furthur expanded to: $$\\hat{\\beta}_1 + \\hat{\\beta}_2 = s; \\hat{\\beta}_1 \\geq 0; \\hat{\\beta}_2 \\geq 0$$ and $$\\hat{\\beta}_1 + \\hat{\\beta}_2 = -s; \\hat{\\beta}_1 \\leq 0; \\hat{\\beta}_2 \\leq 0$$\nQuestion 2 Here, in this exercise, we will generate simulated data and then will use this data to perform best model selection. Firstly, we generate a predictor X of length n=100, as well as a noise vector ϵ of length n=100 such that ϵ = 0.1 * rnorm()\nset.seed(19) # Generating X vector X \u0026lt;- rnorm(100) # Generating noise vector noise_vec \u0026lt;- 0.1 * rnorm(100)  Part a: Here, we want to generate Y of length n = 100 according to the model: Y = β0 + β1X + β2X2 + β3X3 + ϵ\nIn the question we are provided that β0, β1, β2 and β3 are constants such that β0 = 1.0, β1 = −0.1, β2 = 0.05 and β3 = 0.75\n# Setting the coefficients b_0 \u0026lt;- 1 b_1 \u0026lt;- -0.1 b_2 \u0026lt;- 0.05 b_3 \u0026lt;- 0.75 # Generating the response vector Y Y \u0026lt;- b_0 + b_1*(X) + b_2*(X^2) + b_3*(X^3) + noise_vec # We will just have a look at the first five elements of Y to make sure things are in place head(Y) ## [1] 0.006657125 1.258462915 1.133378061 0.960502028 1.704702553 1.107916455  Part b: Here, we want to use regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X2,\u0026hellip;,X8 using the measures Cp, BIC and Adjusted R2.\nmod \u0026lt;- regsubsets(Y~poly(X,8,raw = TRUE), data = data.frame(X,Y), nvmax = 8) # summary of the regsubsets() gives us the best model for each number of predictor # and its respective characteristics including Cp, BIC, RSS, Adjusted R-squared reg.summary \u0026lt;- summary(mod) par(mfrow=c(2,2)) # Finding the index for which we have minimum Cp as for determining best model, # we select the one with lowest Cp cp_min \u0026lt;- which.min(reg.summary$cp) # Plotting Cp vs number of predictors plot(reg.summary$cp, xlab=\u0026quot;Number of Predictors\u0026quot;, ylab=\u0026quot;Best Subset Cp\u0026quot;, type=\u0026quot;l\u0026quot;) # Highlighting the point for which we have minimum Cp points(cp_min, reg.summary$cp[cp_min], col=\u0026quot;red\u0026quot;, pch=16) # Finding the index for which we have minimum BIC as for determining the best model, # we select the one with lowest BIC bic_min \u0026lt;- which.min(reg.summary$bic) # Plotting BIC vs number of predictors plot(reg.summary$bic, xlab=\u0026quot;Number of Predictors\u0026quot;, ylab=\u0026quot;Best Subset BIC\u0026quot;, type=\u0026quot;l\u0026quot;) # Highlighting the point for which we have minimum BIC points(bic_min, reg.summary$bic[bic_min], col=\u0026quot;red\u0026quot;, pch=16) # Finding the index for which we have maximum Adjusted R-squared as for determining # the best model, we select the one with maximum Adjusted R-squared adjr2_max \u0026lt;- which.max(reg.summary$adjr2) # Plotting Adjusted R-squared vs number of predictors plot(reg.summary$adjr2, xlab=\u0026quot;Number of Predictors\u0026quot;, ylab=\u0026quot;Best Subset Adjusted R^2\u0026quot;, type=\u0026quot;l\u0026quot;) # Highlighting the point for which we have maximum adjusted R-squared points(adjr2_max, reg.summary$adjr2[adjr2_max], col=\u0026quot;red\u0026quot;, pch=16)  Now that we have this, we want to know model coefficients from each of Cp, BIC and Adjusted R2.\nknitr::kable(coef(mod, cp_min), caption = \u0026quot;Coefficients for model based on Cp\u0026quot;)   Coefficients for model based on Cp   x    (Intercept) 1.0066155  poly(X, 8, raw = TRUE)1 -0.0839368  poly(X, 8, raw = TRUE)2 0.0576914  poly(X, 8, raw = TRUE)3 0.7496948    knitr::kable(coef(mod, bic_min), caption = \u0026quot;Coefficients for model based on BIC\u0026quot;)   Coefficients for model based on BIC   x    (Intercept) 1.0066155  poly(X, 8, raw = TRUE)1 -0.0839368  poly(X, 8, raw = TRUE)2 0.0576914  poly(X, 8, raw = TRUE)3 0.7496948    knitr::kable(coef(mod, adjr2_max), caption = \u0026quot;Coefficients for model based on Adjusted R-squared\u0026quot;)   Coefficients for model based on Adjusted R-squared   x    (Intercept) 1.0039223  poly(X, 8, raw = TRUE)1 -0.1087415  poly(X, 8, raw = TRUE)2 0.0618556  poly(X, 8, raw = TRUE)3 0.7696595  poly(X, 8, raw = TRUE)5 -0.0026123    Based on the above table, we observe that if we select our model based on Cp, our intercept will be 1.0066, coefficient of X will be -0.0839, that of X2 will be 0.0577 and that of X3 will be 0.7498.\nIf we select our model based on BIC, our intercept will be 1.0066, coefficient of X will be -0.0839, that of X2 will be 0.0577 and that of X3 will be 0.7498.\nFinally, if we select our model based on Adjusted R-squared, our intercept will be 1.0039, coefficient of X will be -0.1087, that of X2 will be 0.0619, that of X3 will be 0.7697 and that of X5 will be -0.0026.\nNow, here, it may seem a bit strange to see Adjusted R-squared selecting 4 predictors. However, if we see at its plot, we can see that after you select 3 predictors, there is not a substantial improvement in adjusted R-squared as you add another predictor. So, we can potentially have model with 4 predictors as well.\nPart c: Here, we want to fit a ridge regression model to the simulated data, again using X, X2, \u0026hellip; , X8 as predictors.\nFirstly, we want to plot the extracted coefficients as a function of log(λ) with a legend containing each curve color and its predictor name in the top right corner.\nHere, in the plot, the numbers on the top scale are the number of coefficients at that weight that are not zero.\nIn the legend \u0026ldquo;poly(x, 8, raw = T)i\u0026rdquo; refers to Xi. So, the corresponding lines represent the coefficients of Xi for that particular value of log(lambda).\n# Getting data into a data frame data_final = data.frame(y = Y, x = X) # Now, we use model.matrix() to create x. It will generate a matrix corresponding to our # 8 predictors. Also, if there are any qualitative variables, it will transform them # into dummy variables as glmnet() can only take numerical, quantitative inputs xmat = model.matrix(y ~ poly(x, 8, raw = T), data = data_final)[, -1] # Applying ridge regression mod.ridge = glmnet(xmat, Y, alpha = 0) # We specify \u0026quot;xvar = lambda\u0026quot; as by default, it plots coefficients against l1 norm plot(mod.ridge, xvar = \u0026quot;lambda\u0026quot;, label = TRUE) legend(\u0026quot;topright\u0026quot;, lwd = 1, col = 1:8, legend = colnames(xmat), cex = .5)  Now that we have the plot, we want to plot the cross-validation error as a function of log(λ) to find optimal λ. Following that, we also want coefficient estimates for that particular value of λ.\n# Setting the seed set.seed(20) # Running the cross validation model with 10 folds by default. mod_cv_2 = cv.glmnet(xmat, Y, alpha = 0) # Plotting Cross Validation model automatically plots cross-validation error as # a function of log(lambda). Here, there are two lambda's indicated by vertical line. # One is minimum mean cross validation error. Other one gives most regularized model # such that error is within one standard error of minimum plot(mod_cv_2, xvar = \u0026quot;lambda\u0026quot;, label = TRUE)  # To find the optimal lambda, we extract the one with minimum mean cross validation error. mod_cv_2$lambda.min ## [1] 2.563188 # Getting the coefficient estimates for optimal lambda x \u0026lt;- (predict(mod.ridge,s = mod_cv_2$lambda.min, type = \u0026quot;coefficients\u0026quot;)) knitr::kable(as.data.frame(as.matrix(x)), caption = \u0026quot;Coefficient for Ridge Regression model with optimal lambda\u0026quot;)   Coefficient for Ridge Regression model with optimal lambda   1    (Intercept) 1.0123824  poly(x, 8, raw = T)1 0.5445252  poly(x, 8, raw = T)2 -0.0050818  poly(x, 8, raw = T)3 0.1891529  poly(x, 8, raw = T)4 0.0031130  poly(x, 8, raw = T)5 0.0241447  poly(x, 8, raw = T)6 0.0011441  poly(x, 8, raw = T)7 0.0023368  poly(x, 8, raw = T)8 0.0002151    Part d: Here, we want to fit a lasso regression model to the simulated data, again using X, X2, \u0026hellip; , X8 as predictors.\nFirstly, we want to plot the extracted coefficients as a function of log(λ) with a legend containing each curve color and its predictor name in the top right corner.\nHere, in the plot, the numbers on the top scale are the number of coefficients at that weight that are not zero.\nIn the legend \u0026ldquo;poly(x, 8, raw = T)i\u0026rdquo; refers to Xi. So, the corresponding lines represent the coefficients of Xi for that particular value of log(lambda).\n# Getting data into a data frame data_final = data.frame(y = Y, x = X) # Now, we use model.matrix() to create x. It will generate a matrix corresponding to our # 8 predictors. Also, if there are any qualitative variables, it will transform them # into dummy variables as glmnet() can only take numerical, quantitative inputs. x_mat = model.matrix(y ~ poly(x, 8, raw = T), data = data_final)[, -1] # Applying lasso regression mod.lasso = glmnet(x_mat, Y, alpha = 1) # We specify \u0026quot;xvar = lambda\u0026quot; as by default, it plots coefficients against l1 norm. plot(mod.lasso, xvar = \u0026quot;lambda\u0026quot;, ylim = c(0,1), label = TRUE) legend(\u0026quot;topright\u0026quot;, lwd = 1, col = 1:8, legend = colnames(x_mat), cex = 0.4)  Now that we have the plot, we want to plot the cross-validation error as a function of log(λ) to find optimal λ. Following that, we also want coefficient estimates for that particular value of λ.\n# Setting the seed set.seed(21) # Running the cross validation model with 10 folds by default mod_cv_L = cv.glmnet(xmat, Y, alpha = 1) # Plotting cross-validation model automatically plots cross-validation error as # a function of log(lambda). Here, there are two lambda's indicated by vertical line. # One is minimum mean cross validation error. Other one gives most regularized model # such that error is within one standard error of minimum. plot(mod_cv_L, xvar = \u0026quot;lambda\u0026quot;, label = TRUE)  # To find the optimal lambda, we extract the one with minimum mean cross validation error mod_cv_L$lambda.min ## [1] 0.02178078 # Getting the coefficient estimates for optimal lambda y \u0026lt;- predict(mod.lasso,s = mod_cv_L$lambda.min, type = \u0026quot;coefficients\u0026quot;) knitr::kable(as.data.frame(as.matrix(y)), caption = \u0026quot;Coefficient for Lasso Regression model with optimal lambda\u0026quot;)   Coefficient for Lasso Regression model with optimal lambda   1    (Intercept) 1.0200232  poly(x, 8, raw = T)1 0.0000000  poly(x, 8, raw = T)2 0.0433211  poly(x, 8, raw = T)3 0.7055455  poly(x, 8, raw = T)4 0.0000000  poly(x, 8, raw = T)5 0.0036491  poly(x, 8, raw = T)6 0.0000000  poly(x, 8, raw = T)7 0.0000000  poly(x, 8, raw = T)8 0.0000000    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6cd833aafe70c1284f7c604e31a35876","permalink":"/courses/example/assignment_5_updated/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/assignment_5_updated/","section":"courses","summary":"Question 1 Here, we want to explore the fact that ridge regression tends to give similar coefficient values to correlated variables, whereas lasso may give quite different coefficient values to correlated variables.","tags":null,"title":"Assignment 5","type":"docs"},{"authors":null,"categories":null,"content":"Question 1: Part a: Here, for all x ≤ ξ for the case of one knot, we want to find f1(x)=a1 + b1x + c1x2 + d1x3 such that f1(x)=f(x).\nHere, f(x)=β0 + β1x + β2x2 + β3x3 + β4(x − ξ)+3. Because, x ≤ ξ, we know that (x − ξ)+3 = 0\nSo, we can transform f1(x)=a1 + b1x + c1x2 + d1x3 into f(x)=β0 + β1x + β2x2 + β3x3 when $$ {\\boxed{{a_1} = {\\beta _0},{b_1} = {\\beta _1},{c_1} = {\\beta _2},{d_1} = {\\beta _3}}} $$\nPart b Here, we want to do the same for all x \u0026gt; ξ, $$ \\begin{aligned}{} f(x) \u0026amp;= {\\beta _0} + {\\beta _1}x + {\\beta _2}{x^2} + {\\beta _3}{x^3} + {\\beta _4}{(x - \\xi )^3}(\\because x \u0026gt; \\xi)\\\\\n\u0026amp;= {\\beta _0} + {\\beta _1}x + {\\beta _2}{x^2} + {\\beta _3}{x^3} + {\\beta _4}({x^3} + 3{\\xi ^2}x - 3\\xi {x^2} - {\\xi ^3})\\\\\n\u0026amp;= {\\beta _0} + {\\beta _1}x + {\\beta _2}{x^2} + {\\beta _3}{x^3} + {\\beta _4}{x^3} + 3{\\xi ^2}{\\beta _4}x - 3\\xi {\\beta _4}{x^2} - {\\xi ^3}{\\beta _4}\\\\\n\u0026amp;= {\\beta _0} - {\\xi ^3}{\\beta _4} + ({\\beta _1} + 3{\\xi ^2}{\\beta _4})x + ({\\beta _2} - 3\\xi {\\beta _4}){x^2} + ({\\beta _3} + {\\beta _4}){x^3} \\end{aligned} $$ Therefore, above equation can be written as f2(x)=a2 + b2x + c2x2 + d2x3, when $$ {\\boxed{{a_2} = {\\beta _0} - {\\xi ^3}{\\beta _4},{b_2} = {\\beta _1} + 3{\\xi ^2}{\\beta _4},{c_2} = {\\beta _2} - 3\\xi {\\beta _4},{d_2} = {\\beta _3} + {\\beta _4}}} $$\nPart c Here, we want to show that f1(ξ)=f2(ξ) and therefore, f is continuous at ξ.\nWhen x = ξ, we have: $$ \\begin{aligned}{} {f_2}(\\xi) \u0026amp;= {\\beta _0} - {\\xi ^3}{\\beta _4} + ({\\beta _1} + 3{\\xi ^2}{\\beta _4})\\xi + ({\\beta _2} - 3\\xi {\\beta _4}){\\xi ^2} + ({\\beta _3} + {\\beta _4}){\\xi ^3}\\\\\n\u0026amp;= {\\beta _0} - {\\xi ^3}{\\beta _4} + {\\beta _1}\\xi + 3{\\xi ^3}{\\beta _4} + {\\beta _2}{\\xi ^2} - 3{\\xi ^3}{\\beta _4} + {\\beta _3}{\\xi ^3} + {\\beta _4}{\\xi ^3}\\\\\n\u0026amp;= {\\beta _0} + {\\beta _1}\\xi + {\\beta _2}{\\xi ^2} + {\\beta _3}{\\xi ^3}\\\\\n\u0026amp;= {f_1}(\\xi) \\end{aligned} $$\nPart d Here, we want to show that f′1(ξ)=f′2(ξ) and therefore, f\u0026rsquo; is continuous at ξ\nWhen x = ξ, we have: $$ \\begin{aligned}{} {{f\u0026rsquo;}_1}(x) \u0026amp;= {\\beta _1} + 2{\\beta _2}x + 3{\\beta _3}{x^2}\\\\\n{{f\u0026rsquo;}_1}(\\xi ) \u0026amp;= {\\boxed{{\\beta _1} + 2{\\beta _2}\\xi + 3{\\beta _3}{\\xi ^2}}} \\end{aligned} $$ and similarly, we have:\n$$ \\begin{aligned}{} {{f\u0026rsquo;}_2}(x) \u0026amp;= {\\beta _1} + 3{\\xi ^2}{\\beta _4} + 2({\\beta _2} - 3\\xi {\\beta _4})x + 3({\\beta _3} + {\\beta _4}){x^2}\\\\\n\u0026amp;= {\\beta _1} + 3{\\xi ^2}{\\beta _4} + 2{\\beta _2}x - 6\\xi {\\beta _4}x + 3{\\beta _3}{x^2} + 3{\\beta _4}{x^2}\\\\\n{{f\u0026rsquo;}_2}(\\xi ) \u0026amp;= {\\beta _1} + 2{\\beta _2}\\xi + 3{\\beta _4}{\\xi ^2} - 6{\\beta _4}{\\xi ^2} + 3{\\beta _3}{\\xi ^2} + 3{\\beta _4}{\\xi ^2}\\\\\n\u0026amp;= {\\beta _1} + 2{\\beta _2}\\xi + (3{\\beta _4} - 6{\\beta _4} + 3{\\beta _3} + 3{\\beta _4}){\\xi ^2}\\\\\n\u0026amp;= {\\boxed{{\\beta _1} + 2{\\beta _2}\\xi + 3{\\beta _3}{\\xi ^2}}} \\end{aligned} $$ Therefore, f′1(ξ)=f′2(ξ).\nPart e Here, we want to show that f″1(ξ)=f″2(ξ) and therefore, f″ is continuous at ξ\nTaking derivatives of f1(x), we have: $$ \\begin{aligned}{} {f_1}(x) \u0026amp;= {a_1} + {b_1}x + {c_1}{x^2} + {d_1}{x^3}\\\\\n{{f\u0026rsquo;}_1}(x) \u0026amp;= {b_1} + 2{c_1}x + 3{d_1}{x^2}\\\\\n{{f\u0026rsquo;'}_1}(x) \u0026amp;= 2{c_1} + 6{d_1}x \\end{aligned} $$ By substituting c1 = β2and d1 = β3 into above equation with x = ξ: $$ {{f\u0026rsquo;'}_1}(\\xi) = {\\boxed{2{\\beta _2} + 6{\\beta _3}\\xi}} $$ Similarily, f″2(x)=2c2 + 6d2x Again, by substituting c2 = β2 − 3ξ**β4 and d2 = β3 + β4 into above equation with x = ξ, we have: $$ \\begin{aligned}{} {{f\u0026rsquo;'}_2}(\\xi) \u0026amp;= 2{\\beta _2} - 6\\xi {\\beta _4} + 6({\\beta _3} + {\\beta _4})\\xi\\\\\n\u0026amp;= {\\boxed{2{\\beta _2} + 6{\\beta _3}\\xi}} \\end{aligned} $$ Therefore, we have f″1(ξ)=f″2(ξ).\nQuestion 2: Part a: Based on the given functions, when λ → ∞, $\\hat g_2$ will have higher order polynomial because of the order of penalty term. As a result, it will be more flexible resulting in a smaller bias and smaller training RSS as compared to $\\hat g_1$.\nPart b: With λ → ∞, as $\\hat g_2$ will have higher order polynomial because of the order of penalty term, it may overfit the data. That will probably result in $\\hat g_1$ having smaller test RSS compared to $\\hat g_2$.\nThis is becuase higher order derivatives bring less information of the function. So, there exists a certain point where the decrease in bias cannot compensate the increase in variance. So, when λ → ∞, the increase in variance using higher order of derivative is more than the decrease in bias, therefore $\\hat g_1$ will have smaller test RSS comparing to $\\hat g_2$.\nPart c: When λ = 0, both models just fit least squares irrespective of the order of derivative used in minimizing $\\hat g$. Therefore, $\\hat g_1$ will have the same training and test RSS as that of $\\hat g_2$.\nQuestion 3: Part a: Classification Error: Classification error is the fraction of training observations in the region that do not belong to the most common class.\nIt is given by: $$E = 1 - \\underset{\\boldsymbol{k}}{max} (\\hat{p}_{mk}) $$\nGini Index: Gini index is the measure of total variance across all classes.\nIt is given by: $$G = \\sum_{k = 1}^{K} \\hat{p}_{mk} (1 - \\hat{p}_{mk})$$\nEntropy: Entropy is an alternative to Gini index and is given by: $$D = - \\sum_{k = 1}^{K}\\hat{p}_{mk} log(\\hat{p}_{mk})$$\nPart b: Here, we want to plot the above three measures\np1 \u0026lt;- seq(0 + 1e-06, 1 - 1e-06, length.out = 100) p2 \u0026lt;- 1 - p1 # error-rate: E \u0026lt;- 1 - apply(rbind(p1, p2), 2, max) # here, 2 indicates to apply the function to columns # Gini index: G \u0026lt;- p1 * (1 - p1) + p2 * (1 - p2) # entropy: D \u0026lt;- -(p1 * log(p1) + p2 * log(p2)) plot(p1, E, type = \u0026quot;l\u0026quot;, col = \u0026quot;black\u0026quot;, xlab = \u0026quot;prob of class 1\u0026quot;, ylab = \u0026quot;value of measure\u0026quot;, ylim = c(min(c(E, G, D)), max(E, G, D))) lines(p1, G, col = \u0026quot;blue\u0026quot;) lines(p1, D, col = \u0026quot;green\u0026quot;) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Classification error\u0026quot;, \u0026quot;Gini index\u0026quot;, \u0026quot;entropy\u0026quot;), col = c(\u0026quot;black\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;green\u0026quot;), lty = c(1, 1), cex = 0.5)  Question 4: We are provided 10 estimates of P(Class is red|X) which are: 0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7,0.75\nSo, the respective estimates of P(Class is green|X) will be: 0.9,0.85,0.8,0.8,0.45,0.4,0.4,0.35,0.3,0.25\nMajority vote approach: Here, we see that 6 out of 10 observations have P(Class is red|X) \u0026gt; P(Class is green|X) i.e., P(Class is red|X) \u0026gt; 0.5. So, the classification would be Red.\nAverage probability approach: Based on these 10 estimates, ${\\hat{P}(R|X)}$ = mean of the estimates of P(Class is red|X). So, ${\\hat{P}(R|X)} = 0.45$ . Therefore, ${\\hat{P}(G|X)} = 0.55$\nSo, the classification would be Green.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"704b162a9405179d19071287a56ea145","permalink":"/courses/example/assignment_6_updated/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/assignment_6_updated/","section":"courses","summary":"Question 1: Part a: Here, for all x ≤ ξ for the case of one knot, we want to find f1(x)=a1 + b1x + c1x2 + d1x3 such that f1(x)=f(x).","tags":null,"title":"Assignment 6","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}}  renders as\n Click to view the spoiler  You found me!    Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Raj Patel"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Raj Patel"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Raj Patel","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","开源"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Raj Patel","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Raj Patel","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]